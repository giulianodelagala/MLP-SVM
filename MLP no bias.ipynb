{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multilayer Perceptron\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = [\"iris\",\n",
    "              \"Enfermedad_Cardiaca\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeerDatos(filename : str, separa : str, header = True):\n",
    "    if (header):\n",
    "        data = pd.read_csv(filename + \".csv\", sep =separa, header = 0)\n",
    "    else:\n",
    "        data = pd.read_csv(filename+ \".csv\", sep = separa, header = None)\n",
    "    #data = data.sample(frac = 1) #shuffle data\n",
    "    data = data.sort_values(data.columns[-1])\n",
    "    return data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalizar_Datos(data : np.array):\n",
    "    #normal = np.empty_like(data)\n",
    "    for i in range (0,np.size(data[0])):\n",
    "        media = np.mean(data[:,i])\n",
    "        desvi =np.std(data[:,i])\n",
    "        data[:,i] = (data[:,i] - media)/desvi\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crear_k_folds(data : np.array , k:int, clases: []):\n",
    "    folds = []\n",
    "    tot_clase = []\n",
    "    prop_clase = [] #Acumulado de indices\n",
    "    pre_fold = []\n",
    "\n",
    "    m = np.size(data[:,-1]) #numero de datos\n",
    "    #n = np.size(data[0])\n",
    "    for i in clases:\n",
    "        tot_clase.append(np.count_nonzero( data[:,-1] == i))\n",
    "\n",
    "    prop_clase.append(tot_clase[0])\n",
    "    for i in range (1, len(tot_clase)):\n",
    "        prop_clase.append( prop_clase[i-1] + tot_clase[i])\n",
    "\n",
    "    pos_ini = 0\n",
    "    for i in range(0, len(clases)):\n",
    "        pre_fold.append(np.array_split(data[pos_ini:prop_clase[i]], k))\n",
    "        pos_ini = prop_clase[i]\n",
    "    \n",
    "    for i in range (0,k):\n",
    "        temp = np.empty( (0,np.size(data[0])) )\n",
    "        for j in range(0,len(clases)):\n",
    "            temp = np.vstack( (temp,pre_fold[j][i]))\n",
    "        folds.append(temp)\n",
    "            \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoidal(X:np.array, theta:np.array):\n",
    "    pot = X.dot(theta)\n",
    "    return 1/(1+ np.exp(-pot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds(D : np.array):\n",
    "    return D*(1-D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calcular_Funcion_Costo(X: np.array, y:np.array):\n",
    "    #J(theta) = -1/m[ SUM( y* log(h(x)) + (1-y)*log(1-h(x)))\n",
    "    m = np.size(X[:,0]) #numero de datos\n",
    "    costo = 0\n",
    "    for i in range(0, len(X[0])):\n",
    "        costo += -1/m * ( np.sum( y[i].dot(np.log(X[i])) + (1-y[i]).dot( np.log(1-X[i]))) )\n",
    "    return costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerarW( num_capas : int, dim_capas = []):\n",
    "    W = {}\n",
    "    for i in range(0,num_capas):\n",
    "        temp = np.random.randn( dim_capas[i], dim_capas[i+1] )\n",
    "        W[i] = temp\n",
    "    return W\n",
    "\n",
    "def GenerarBias(num_capas:int, dim_capas = []):\n",
    "    B = {}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forward (X: np.array, W : {} ):\n",
    "    A = {}\n",
    "    h_l = X\n",
    "    A[0] = h_l\n",
    "    for i in range(0, len(W)):\n",
    "        h_l = Sigmoidal(h_l, W[i])\n",
    "        A[i+1] = h_l   #A = __h\n",
    "    return A\n",
    "def Backward (X: np.array, y: np.array, W:{}, A:{}, tasa_apren:float):\n",
    "    #Actualizacion de W (pesos) de la red por back-propagation   \n",
    "    #deriv J(theta) = a^l* delta^(l+1)\n",
    "    #g'(z) = a * (1-a)\n",
    "    m = np.size(X[:,-1])\n",
    "    delta_t = (A[len(A)-1] - y) #* ds(A[len(A)-1])\n",
    "    for i in range(len(W)-1,-1,-1):\n",
    "        #delta = theta.delta * g'(z)\n",
    "        W[i]-= tasa_apren* (A[i].T.dot(delta_t))/ m\n",
    "        if (i != 0):\n",
    "            delta_t = ds(A[i])*(delta_t.dot(W[i].T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradiente_Descendiente(X: np.array, y:np.array, W:{},\n",
    "                          num_itera:int, tasa_apren:float):\n",
    "    arr_costo = np.empty(num_itera, dtype =float)\n",
    "    A = {}\n",
    "    num_capas = len(W)\n",
    " \n",
    "    for it in range(0, num_itera):\n",
    "        A = Forward(X, W)\n",
    "        arr_costo[it] = Calcular_Funcion_Costo(A[num_capas], y)\n",
    "        Backward(X, y, W, A, tasa_apren)      \n",
    "    return A[num_capas], arr_costo, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransformacionOneShot(y: np.array, clases:[]):\n",
    "    num_clases = len(clases)\n",
    "    vec_clases = np.empty((0,num_clases), dtype = int)\n",
    "    for i in y:\n",
    "        idx = clases.index(i)\n",
    "        vec = [0] * num_clases\n",
    "        vec[idx] = 1\n",
    "        vec_clases = np.vstack ((vec_clases, vec))\n",
    "    return vec_clases\n",
    "\n",
    "def OneShot_Salida(y:np.array):\n",
    "    y_cat = np.zeros_like(y)\n",
    "    max = np.argmax(y, axis = 1)\n",
    "    for i in range(0, len(max)):\n",
    "        y_cat[i,max[i]] = 1\n",
    "    return y_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calcular_Accuracy(X:np.array, y:np.array, theta:np.array):\n",
    "    y_calc = Forward(X, theta)\n",
    "    y_calc = OneShot_Salida(y_calc[len(y_calc)-1])\n",
    "    aciertos = 0\n",
    "    \n",
    "    for i in (y - y_calc):\n",
    "        if (np.count_nonzero(i) == 0):\n",
    "            aciertos += 1\n",
    "    return aciertos/np.size(y[:,0])\n",
    "    \n",
    "def PromedioAccuracy(test:np.array, theta, k, clases):\n",
    "    accu = np.zeros(k)\n",
    "    for i in range(0,k):\n",
    "        X_test = test[i][:,:-1]\n",
    "        X_test = X_test.astype('float64')\n",
    "        X_test = Normalizar_Datos(X_test)\n",
    "\n",
    "        y_test = TransformacionOneShot(test[i][:,-1], clases)\n",
    "\n",
    "        accu[i] = Calcular_Accuracy(X_test, y_test, theta)\n",
    "    return accu.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculoParametros(folds:[], k:int, iteraciones:int, alpha:float,\n",
    " num_clases:int, num_capa_hidden:int, num_neurona: int, clases:[]):\n",
    "    arr_costo = []\n",
    "    arr_theta = []\n",
    "    arr_test = []\n",
    "    for test_i in range(0, k):\n",
    "        test = folds[test_i] \n",
    "        train = np.zeros( (0,np.size(folds[0][0])) )\n",
    "        for train_i in range (0, k):         \n",
    "            if (train_i == test_i):\n",
    "                continue\n",
    "            else:\n",
    "                train = np.vstack( (train,folds[train_i]) )\n",
    "            \n",
    "        costo = []\n",
    "        X_train = train[:,:-1]\n",
    "        X_train = X_train.astype('float64')\n",
    "        X_train = Normalizar_Datos(X_train)\n",
    "\n",
    "        N = np.size(X_train[:,-1]) #tama√±o batch\n",
    "        D_in = np.size(X_train[0]) #dimension entrada\n",
    "        D_out = num_clases\n",
    "\n",
    "        #Generacion array de capas\n",
    "        array_capas = []\n",
    "        array_capas.append(D_in)\n",
    "        for i in range(0, num_capa_hidden):\n",
    "            array_capas.append(num_neurona)\n",
    "        array_capas.append(D_out)\n",
    "\n",
    "        W = GenerarW(num_capa_hidden +1, array_capas)\n",
    "\n",
    "        y_train = TransformacionOneShot( train[:,-1], clases)\n",
    "\n",
    "        theta, costo, W = Gradiente_Descendiente(X_train, y_train, W, iteraciones, alpha)\n",
    "        arr_theta.append(theta)\n",
    "        arr_costo.append(costo)\n",
    "        arr_test.append(test)\n",
    "      \n",
    "    return theta, arr_costo, arr_test, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BusquedaParametros(folds:[], k, num_clases, clases:[]):\n",
    "    alpha = [0.1, 0.2, 0.4, 0.5]\n",
    "    iteraciones = range(500,3501,500)\n",
    "    num_capa = [1,2,3]\n",
    "    num_neurona = range(5,20,5)\n",
    "    arr_accu = np.empty( (len(alpha),len(iteraciones) ))\n",
    "    for nc in num_capa:\n",
    "        print (\"############################\")\n",
    "        print (\"Numero de hidden layers: \", nc)\n",
    "        print (\"############################\")\n",
    "        for nn in num_neurona:\n",
    "            print (\"Numero de neuronas x layer: \", nn)\n",
    "            for tasa in range(0,len(alpha)):\n",
    "                for it in range(0, len(iteraciones)):\n",
    "                    theta, dummy, test, W = CalculoParametros(folds, k, iteraciones[it], alpha[tasa], num_clases, nc, nn, clases)\n",
    "\n",
    "                    arr_accu[tasa,it] = PromedioAccuracy(test, W, k, clases)\n",
    "\n",
    "            print(pd.DataFrame(arr_accu, index = alpha, columns = iteraciones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = LeerDatos(dataset[0], separa = ',')\n",
    "clases = [\"Setosa\", \"Versicolor\", \"Virginica\"]\n",
    "iris_folds = Crear_k_folds(iris, 3, clases)\n",
    "#theta, arr_costo, arr_test = CalculoParametros(iris_folds, 3, 500, 0.1, 3, 3, 20, clases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "############################\nNumero de hidden layers:  1\n############################\nNumero de neuronas x layer:  5\n          500      1000      1500      2000      2500      3000      3500\n0.1  0.839461  0.893791  0.920752  0.887255  0.913399  0.920752  0.906863\n0.2  0.906454  0.906863  0.933415  0.933415  0.959967  0.966503  0.959967\n0.4  0.893791  0.939951  0.966503  0.973039  0.973039  0.973039  0.973039\n0.5  0.906863  0.940359  0.973039  0.973039  0.973039  0.973448  0.979984\nNumero de neuronas x layer:  10\n          500      1000      1500      2000      2500      3000      3500\n0.1  0.886846  0.907271  0.920343  0.926879  0.940359  0.940359  0.940359\n0.2  0.900735  0.933415  0.933415  0.953431  0.973039  0.966503  0.966503\n0.4  0.926879  0.953431  0.959967  0.973039  0.973039  0.979984  0.973039\n0.5  0.933415  0.959967  0.966503  0.973039  0.979984  0.979984  0.979984\nNumero de neuronas x layer:  15\n          500      1000      1500      2000      2500      3000      3500\n0.1  0.893382  0.907271  0.933824  0.946895  0.940359  0.953431  0.959967\n0.2  0.913807  0.933824  0.946895  0.953431  0.966503  0.973039  0.973039\n0.4  0.940359  0.959967  0.966503  0.973039  0.979984  0.973039  0.973039\n0.5  0.940359  0.966503  0.973039  0.973039  0.979984  0.973039  0.979984\n############################\nNumero de hidden layers:  2\n############################\nNumero de neuronas x layer:  5\n          500      1000      1500      2000      2500      3000      3500\n0.1  0.879493  0.839461  0.880310  0.966503  0.973039  0.973039  0.973039\n0.2  0.973039  0.979984  0.973039  0.973039  0.979984  0.979984  0.979984\n0.4  0.973039  0.979984  0.979984  0.979984  0.979984  0.986520  0.979984\n0.5  0.966503  0.979984  0.986520  0.979575  0.986520  0.986520  0.986520\nNumero de neuronas x layer:  10\n          500      1000      1500      2000      2500      3000      3500\n0.1  0.919935  0.959967  0.973039  0.973039  0.979984  0.979984  0.979984\n0.2  0.966503  0.973039  0.979984  0.979984  0.973039  0.979984  0.973039\n0.4  0.973039  0.979984  0.979984  0.986520  0.979575  0.986520  0.979575\n0.5  0.979984  0.979984  0.979984  0.986520  0.986520  0.986520  0.979575\nNumero de neuronas x layer:  15\n          500      1000      1500      2000      2500      3000      3500\n0.1  0.973039  0.966503  0.973039  0.979984  0.973039  0.979984  0.979984\n0.2  0.966503  0.973039  0.973039  0.979984  0.979984  0.973039  0.979575\n0.4  0.979984  0.979984  0.979984  0.986520  0.986520  0.979575  0.986520\n0.5  0.979984  0.986520  0.986520  0.986520  0.986520  0.979575  0.986520\n############################\nNumero de hidden layers:  3\n############################\nNumero de neuronas x layer:  5\n          500      1000      1500      2000      2500      3000      3500\n0.1  0.726716  0.839461  0.973039  0.926471  0.979984  0.979984  0.979984\n0.2  0.973039  0.966912  0.979984  0.979984  0.979984  0.986520  0.979575\n0.4  0.979984  0.973039  0.979575  0.979575  0.973039  0.979575  0.986520\n0.5  0.973039  0.979984  0.973039  0.979575  0.979575  0.979575  0.979575\nNumero de neuronas x layer:  10\n          500      1000      1500      2000      2500      3000      3500\n0.1  0.913399  0.973039  0.979984  0.979984  0.973039  0.973039  0.979984\n0.2  0.966503  0.979984  0.973039  0.979984  0.986520  0.986520  0.979575\n0.4  0.973039  0.979575  0.986520  0.986520  0.973039  0.979575  0.973039\n0.5  0.979984  0.986520  0.979575  0.973039  0.979575  0.979575  0.973039\nNumero de neuronas x layer:  15\n          500      1000      1500      2000      2500      3000      3500\n0.1  0.933415  0.973039  0.979984  0.979984  0.979984  0.973039  0.986520\n0.2  0.979984  0.979984  0.979984  0.979984  0.973039  0.979575  0.979575\n0.4  0.979984  0.979575  0.986520  0.986520  0.986520  0.973039  0.973039\n0.5  0.979984  0.979575  0.986520  0.979575  0.986111  0.986111  0.979575\n"
    }
   ],
   "source": [
    "BusquedaParametros(iris_folds, 3, num_clases = 3, clases = clases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardiaca = LeerDatos(dataset[1], separa = '\\t')\n",
    "clas_cardiaca = [0,1]\n",
    "cardiaca_folds = Crear_k_folds(cardiaca, 3, clas_cardiaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Numero de hidden layers:  1\nNumero de neuronas x capa:  5\n          500      1000      1500      2000      2500      3000      3500\n0.1  0.838284  0.871287  0.874587  0.871287  0.884488  0.897690  0.897690\n0.2  0.854785  0.867987  0.877888  0.874587  0.897690  0.877888  0.881188\n0.4  0.871287  0.897690  0.887789  0.900990  0.907591  0.867987  0.864686\n0.5  0.858086  0.900990  0.874587  0.900990  0.917492  0.904290  0.874587\nNumero de neuronas x capa:  10\n          500      1000      1500      2000      2500      3000      3500\n0.1  0.831683  0.867987  0.874587  0.910891  0.897690  0.884488  0.897690\n0.2  0.871287  0.881188  0.881188  0.900990  0.874587  0.907591  0.914191\n0.4  0.864686  0.884488  0.917492  0.924092  0.937294  0.894389  0.900990\n0.5  0.891089  0.900990  0.917492  0.914191  0.914191  0.920792  0.910891\nNumero de neuronas x capa:  15\n          500      1000      1500      2000      2500      3000      3500\n0.1  0.858086  0.858086  0.874587  0.884488  0.891089  0.930693  0.910891\n0.2  0.881188  0.867987  0.884488  0.910891  0.924092  0.927393  0.897690\n0.4  0.884488  0.894389  0.937294  0.917492  0.933993  0.920792  0.914191\n0.5  0.884488  0.914191  0.927393  0.937294  0.930693  0.937294  0.894389\nNumero de hidden layers:  2\nNumero de neuronas x capa:  5\n          500      1000      1500      2000      2500      3000      3500\n0.1  0.772277  0.864686  0.871287  0.867987  0.881188  0.881188  0.900990\n0.2  0.851485  0.881188  0.884488  0.871287  0.910891  0.904290  0.877888\n0.4  0.874587  0.887789  0.914191  0.904290  0.881188  0.907591  0.900990\n0.5  0.887789  0.871287  0.900990  0.887789  0.910891  0.858086  0.894389\nNumero de neuronas x capa:  10\n          500      1000      1500      2000      2500      3000      3500\n0.1  0.811881  0.884488  0.867987  0.881188  0.884488  0.897690  0.904290\n0.2  0.887789  0.884488  0.887789  0.910891  0.904290  0.904290  0.910891\n0.4  0.904290  0.910891  0.900990  0.933993  0.904290  0.914191  0.927393\n0.5  0.891089  0.917492  0.894389  0.907591  0.924092  0.907591  0.897690\nNumero de neuronas x capa:  15\n          500      1000      1500      2000      2500      3000      3500\n0.1  0.864686  0.884488  0.894389  0.881188  0.917492  0.907591  0.891089\n0.2  0.858086  0.904290  0.927393  0.930693  0.920792  0.920792  0.943894\n0.4  0.894389  0.914191  0.900990  0.943894  0.920792  0.904290  0.914191\n0.5  0.897690  0.920792  0.924092  0.930693  0.924092  0.930693  0.933993\nNumero de hidden layers:  3\nNumero de neuronas x capa:  5\n          500      1000      1500      2000      2500      3000      3500\n0.1  0.755776  0.792079  0.848185  0.881188  0.891089  0.897690  0.884488\n0.2  0.861386  0.897690  0.867987  0.881188  0.884488  0.871287  0.891089\n0.4  0.871287  0.894389  0.917492  0.900990  0.891089  0.907591  0.907591\n0.5  0.881188  0.897690  0.871287  0.881188  0.894389  0.910891  0.887789\nNumero de neuronas x capa:  10\n          500      1000      1500      2000      2500      3000      3500\n0.1  0.811881  0.867987  0.877888  0.917492  0.920792  0.891089  0.904290\n0.2  0.881188  0.884488  0.917492  0.907591  0.924092  0.910891  0.914191\n0.4  0.910891  0.907591  0.900990  0.924092  0.914191  0.927393  0.920792\n0.5  0.917492  0.904290  0.924092  0.891089  0.914191  0.904290  0.894389\nNumero de neuronas x capa:  15\n          500      1000      1500      2000      2500      3000      3500\n0.1  0.838284  0.877888  0.887789  0.914191  0.917492  0.904290  0.904290\n0.2  0.907591  0.917492  0.907591  0.907591  0.937294  0.927393  0.920792\n0.4  0.910891  0.924092  0.914191  0.891089  0.930693  0.904290  0.927393\n0.5  0.920792  0.914191  0.927393  0.910891  0.924092  0.950495  0.914191\n"
    }
   ],
   "source": [
    "BusquedaParametros(cardiaca_folds, k=3, num_clases = 2, clases = clas_cardiaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}